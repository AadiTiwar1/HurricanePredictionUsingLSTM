{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33a1f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce35c5",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb7a535",
   "metadata": {},
   "source": [
    "Stock price prediction is one of the most rewarding problems in modern finance, where the accurate forecasting of future stock prices can yield significant profit and reduce the risks. LSTM (Long Short-Term Memory) is a recurrent Neural Network (RNN) applicable to a broad range of problems aiming to analyze or classify sequential data. Therefore, many people have used LSTM to predict the future stock price based on the historical data sequences with great success.\n",
    "\n",
    "On the other hand, recent studies have shown that the LSTM's efficiency and trainability can be improved by replacing some of the layers in the LSTM with variational quantum layers, thus making it a quantum-classical hybrid model of LSTM which we will call QLSTM for Quantum LSTM. In the study done by Samuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L. Fang, they show that QLSTM offers better trainability compared to its classical counterpart as it proved to learn significantly more information after the first training epoch than its classical counterpart, learnt the local features better, all while having a comparable number of parameters. Inspired by these recent results, we proceed to test this variational quantum-classical hybrid neural network technique on stock price predictions. \n",
    "\n",
    "In the following notebook, we show a proof of concept that QLSTM can be used to great effect for stock price prediction, offering comparable and arguably better results than its classical counterpart. To do so, we implement both LSTM and QLSTM to predict the stock prices of the company Merck & Co. Inc (MRK) with the same number of features, of which we chose based on earlier studies done with stock price predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922097fa",
   "metadata": {},
   "source": [
    "This submission was motivated by a combination of a few separate studies:\n",
    "\n",
    "Stock price prediction use BERT and GAN: https://arxiv.org/pdf/2107.09055.pdf, Priyank Sonkiya, Vikas Bajpai, Anukriti Bansal <br>\n",
    "Quantum Long Short-Term Memory: https://arxiv.org/pdf/2009.01783.pdf, Samuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L. Fang <br>\n",
    "\n",
    "With code and ideas reused and repurposed from the following sources:\n",
    "\n",
    "Example of a QLSTM: https://github.com/rdisipio/qlstm, Riccardo Di Sipio <br>\n",
    "How to use PyTorch LSTMs for time series regression: https://www.crosstab.io/articles/time-series-pytorch-lstm, Brian Kent <br>\n",
    "Using GANs to predict stock price movement: https://towardsdatascience.com/aifortrading-2edd6fac689d, Boris Banushev<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696bb7d7",
   "metadata": {},
   "source": [
    "## Brief Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e301a55",
   "metadata": {},
   "source": [
    "To demonstrate the use of QLSTM for stock prediction, we use the stock prices of the company Merck & Co. Inc (MRK). The notebook will proceed in the following manner:\n",
    "\n",
    "1. Brief description of Data\n",
    "2. Using Classical LSTM to perform stock price prediction\n",
    "3. Defining QLSTM and using it to perform stock price prediction\n",
    "4. Comparison between LSTM and QLSTM\n",
    "\n",
    "Note that for the LSTM, we would be using PyTorch; while for the QLSTM we would be using PyTorch and Pennylane. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf2258",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841734de",
   "metadata": {},
   "source": [
    "Data Description: We have collected the historical data of the MRK stock prices, of which we focus on the closing price. Our goal is then to forecast the closing stock prices of MRK using LSTM (or QLSTM). \n",
    "\n",
    "To achieve this goal, we have collected the following necessary data and information. This includes the following:\n",
    "\n",
    "- Technical indicators\n",
    "- Trend approximations (Fourier Transforms)\n",
    "- ARIMA \n",
    "- Correlated assets\n",
    "- Sentimental analysis\n",
    "\n",
    "While interesting and important in its own right, we have decided not to go into detail for the data collection in this notebook. For more information, please go take a look at the Data Collection notebook also in this Github. \n",
    "\n",
    "In this section, our goal is to process the data necessary for the LSTM and QLSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37e1ea",
   "metadata": {},
   "source": [
    "First, we read in the data, dropping the index and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c36071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>max_sustained_wind</th>\n",
       "      <th>central_pressure</th>\n",
       "      <th>tmrw windspeed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1851-06-25</th>\n",
       "      <td>28.2N</td>\n",
       "      <td>96.8W</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851-06-26</th>\n",
       "      <td>28.6N</td>\n",
       "      <td>98.9W</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851-06-27</th>\n",
       "      <td>30.5N</td>\n",
       "      <td>100.1W</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851-06-28</th>\n",
       "      <td>31.0N</td>\n",
       "      <td>100.2W</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851-07-05</th>\n",
       "      <td>22.2N</td>\n",
       "      <td>97.6W</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-09</th>\n",
       "      <td>25.0N</td>\n",
       "      <td>75.7W</td>\n",
       "      <td>45</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-10</th>\n",
       "      <td>31.2N</td>\n",
       "      <td>74.0W</td>\n",
       "      <td>60</td>\n",
       "      <td>993.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-11</th>\n",
       "      <td>37.6N</td>\n",
       "      <td>58.2W</td>\n",
       "      <td>65</td>\n",
       "      <td>980.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-12</th>\n",
       "      <td>41.9N</td>\n",
       "      <td>49.9W</td>\n",
       "      <td>55</td>\n",
       "      <td>983.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-13</th>\n",
       "      <td>40.7N</td>\n",
       "      <td>45.4W</td>\n",
       "      <td>45</td>\n",
       "      <td>987.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9983 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           latitude longitude  max_sustained_wind  central_pressure  \\\n",
       "date                                                                  \n",
       "1851-06-25    28.2N     96.8W                  80               NaN   \n",
       "1851-06-26    28.6N     98.9W                  50               NaN   \n",
       "1851-06-27    30.5N    100.1W                  40               NaN   \n",
       "1851-06-28    31.0N    100.2W                  40               NaN   \n",
       "1851-07-05    22.2N     97.6W                  80               NaN   \n",
       "...             ...       ...                 ...               ...   \n",
       "2015-11-09    25.0N     75.7W                  45            1008.0   \n",
       "2015-11-10    31.2N     74.0W                  60             993.0   \n",
       "2015-11-11    37.6N     58.2W                  65             980.0   \n",
       "2015-11-12    41.9N     49.9W                  55             983.0   \n",
       "2015-11-13    40.7N     45.4W                  45             987.0   \n",
       "\n",
       "            tmrw windspeed  \n",
       "date                        \n",
       "1851-06-25            50.0  \n",
       "1851-06-26            40.0  \n",
       "1851-06-27            40.0  \n",
       "1851-06-28            80.0  \n",
       "1851-07-05            50.0  \n",
       "...                    ...  \n",
       "2015-11-09            60.0  \n",
       "2015-11-10            65.0  \n",
       "2015-11-11            55.0  \n",
       "2015-11-12            45.0  \n",
       "2015-11-13             NaN  \n",
       "\n",
       "[9983 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =            pd.read_csv('atlantic (2).csv')\n",
    "\n",
    "df.drop(['status_of_system', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8','Unnamed: 9', \n",
    "        'Unnamed: 10', 'Unnamed: 11','Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', \n",
    "        'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18'], inplace=True, axis=1)\n",
    "df['date'] = pd.to_datetime(df['date'], format = \"%Y%m%d\").dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df.drop_duplicates(subset=['date'], keep='last', inplace=True)\n",
    "\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "df['tmrw windspeed'] =  df['max_sustained_wind'].shift(-1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaa9b52",
   "metadata": {},
   "source": [
    "We identify the dependent and independent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53abe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"tmrw windspeed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cedb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['central_pressure', 'latitude', 'longitude', 'max_sustained_wind']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(df.columns.difference([\"date\", 'tmrw windspeed']))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65613d0",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7baf43",
   "metadata": {},
   "source": [
    "To process the data, we first split it into training and test data, where two-thirds of the data is used for training, and the last third is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465d24a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot do slice indexing on Index with these indexers [6688] of type int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37712\\3770230888.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.67\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_slice_axis\u001b[1;34m(self, slice_obj, axis)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mslice_indexer\u001b[1;34m(self, start, end, step, kind)\u001b[0m\n\u001b[0;32m   6286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deprecated_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"kind\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"slice_indexer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6288\u001b[1;33m         \u001b[0mstart_slice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_locs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6290\u001b[0m         \u001b[1;31m# return a slice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mslice_locs\u001b[1;34m(self, start, end, step, kind)\u001b[0m\n\u001b[0;32m   6502\u001b[0m         \u001b[0mend_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6504\u001b[1;33m             \u001b[0mend_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_slice_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"right\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6505\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend_slice\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6506\u001b[0m             \u001b[0mend_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_slice_bound\u001b[1;34m(self, label, side, kind)\u001b[0m\n\u001b[0;32m   6405\u001b[0m         \u001b[1;31m# For datetime indices label may be a string that has to be converted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6406\u001b[0m         \u001b[1;31m# to datetime boundary according to its resolution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6407\u001b[1;33m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6409\u001b[0m         \u001b[1;31m# we need to look up the label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_maybe_cast_slice_bound\u001b[1;34m(self, label, side, kind)\u001b[0m\n\u001b[0;32m   6352\u001b[0m         \u001b[1;31m# reject them, if index does not contain label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mis_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6354\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalid_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"slice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot do slice indexing on Index with these indexers [6688] of type int"
     ]
    }
   ],
   "source": [
    "size = int(len(df) * 0.67)\n",
    "\n",
    "df_train = df.loc[:size].copy()\n",
    "df_test = df.loc[size:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc621071",
   "metadata": {},
   "source": [
    "Next, in order to ensure that some values due to their mangnitude do not inherently dominate the features, we standardize their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = df_train[target].mean()\n",
    "target_stdev = df_train[target].std()\n",
    "\n",
    "for c in df_train.columns:\n",
    "    mean = df_train[c].mean()\n",
    "    stdev = df_train[c].std()\n",
    "\n",
    "    df_train[c] = (df_train[c] - mean) / stdev\n",
    "    df_test[c] = (df_test[c] - mean) / stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bb0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Factory import SequenceDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b59fe",
   "metadata": {},
   "source": [
    "Finally, the last step in the data processing to prepare for LSTM is to prepare the data in a sequence of past observations. Preparation of the LSTM on time series data means that it uses a certain number of past observations to predict the future. In this case, the sequence length decides how many days the LSTM considers in advance. If the sequence length is $n$, then the LSTM considers the last $n$ observations to predict the $n+1$th day.\n",
    "\n",
    "We decided the sequence length as 3 for purposes of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "\n",
    "batch_size = 1\n",
    "sequence_length = 3\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    df_train,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    df_test,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245af1d7",
   "metadata": {},
   "source": [
    "# Classical LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a784bc",
   "metadata": {},
   "source": [
    "We first define two functions:\n",
    "    \n",
    "- train_model: function to train the model based on the batches of data\n",
    "- test_model: function to test the model on the testing data\n",
    "    \n",
    "We print the loss at the end to understand how the model is performing with regards to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe496cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def test_model(data_loader, model, loss_function):\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5d66a",
   "metadata": {},
   "source": [
    "## Running the Classical LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7aabb",
   "metadata": {},
   "source": [
    "To understand our implementation of QLSTM, we first explain our implementation LSTM. LSTM follows the following structure:\n",
    "\n",
    "<img src=\"lstm2.jpg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Image taken from: Quantum Long Short-Term Memory, https://arxiv.org/pdf/2009.01783.pdf (Samuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L. Fang (2020)) \n",
    "\n",
    "Simply put, LSTM uses 4 neural network layers in each LSTM cell. They are:\n",
    "\n",
    "- Forget layer\n",
    "- Input layer\n",
    "- Update layer\n",
    "- Output layer\n",
    "\n",
    "We can see the corresponding layers in the W cells in the picture above. We will be skipping the technical details, but it is important to note that these 4 layers are the keys to building an LSTM neural network model that we can train and eventually use to predict. They usually take the form of a normal NN layer (like a linear layer with reLU or convolutional layers).\n",
    "\n",
    "LSTMs are well studied, and there is a native implementation of it in PyTorch to begin with, so we use a slightly modified version of it for the time series LSTM that we perform here. The code for the time series LSTM was reused from:\n",
    "\n",
    "How to use PyTorch LSTMs for time series regression: https://www.crosstab.io/articles/time-series-pytorch-lstm, Brian Kent.\n",
    "\n",
    "In the following code, we train LSTM to predict future stock prices, and then test it on the test dataset. The learning rate of 0.0001 was decided after some experimentation, where we chose the learning rate that gave accurate results. The number of epochs we use is 20, by which it would have converged and thus would suffice for the purposes of this notebook. After that, we visualize three different graphs: the comparison between the real stock prices and the ones given by the model; and the evolution of test loss and training loss by epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0568ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Factory import ShallowRegressionLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a758f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_hidden_units = 16\n",
    "\n",
    "model = ShallowRegressionLSTM(num_sensors=len(features), hidden_units=num_hidden_units)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2cdd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_loss_train = []\n",
    "classical_loss_test = []\n",
    "print(\"Untrained test\\n--------\")\n",
    "test_loss = test_model(test_loader, model, loss_function)\n",
    "print()\n",
    "classical_loss_test.append(test_loss)\n",
    "\n",
    "for ix_epoch in range(20):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "    test_loss = test_model(test_loader, model, loss_function)\n",
    "    print()\n",
    "    classical_loss_train.append(train_loss)\n",
    "    classical_loss_test.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44945e",
   "metadata": {},
   "source": [
    "We then use the model to predict the test set, and then compare the results of the prediction to the real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ffa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    \"\"\"Just like `test_loop` function but keep track of the outputs instead of the loss\n",
    "    function.\n",
    "    \"\"\"\n",
    "    output = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            y_star = model(X)\n",
    "            output = torch.cat((output, y_star), 0)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58531cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "ystar_col = \"Model forecast\"\n",
    "df_train[ystar_col] = predict(train_eval_loader, model).numpy()\n",
    "df_test[ystar_col] = predict(test_loader, model).numpy()\n",
    "\n",
    "df_out = pd.concat((df_train, df_test))[[target, ystar_col]]\n",
    "\n",
    "for c in df_out.columns:\n",
    "    df_out[c] = df_out[c] * target_stdev + target_mean\n",
    "\n",
    "print(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ed8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(range(2718), df_out[\"Close_lead1\"], label = \"Real\")\n",
    "plt.plot(range(2718), df_out[\"Model forecast\"], label = \"LSTM Prediction\")\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xlabel('Days')\n",
    "plt.vlines(size, ymin = 30, ymax = 90, label = \"Test set start\", linestyles = \"dashed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(21), classical_loss_test)\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 21), classical_loss_train)\n",
    "plt.ylabel('Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
